

















class LSTMCell(nn.Module):
    def __init__(self, input_size, hidden_size, bidirectional, device):
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.bidirectional = bidirectional
        self.device = device
        self.setWeights(self.input_size, self.hidden_size)

    def setWeights(self, input_size, hidden_size):
        # Input Gate
        self.W_i = torch.rand(input_size, hidden_size).to(self.device)
        self.U_i = torch.rand(hidden_size, hidden_size).to(self.device)
        self.b_i = torch.rand(hidden_size).to(self.device)

        # Forget Gate
        self.W_f = torch.rand(input_size, hidden_size).to(self.device)
        self.U_f = torch.rand(hidden_size, hidden_size).to(self.device)
        self.b_f = torch.rand(hidden_size).to(self.device)

        # Cell Gate
        self.W_c = torch.rand(input_size, hidden_size).to(self.device)
        self.U_c = torch.rand(hidden_size, hidden_size).to(self.device)
        self.b_c = torch.rand(hidden_size).to(self.device)

        # Output Gate
        self.W_o = torch.rand(input_size, hidden_size).to(self.device)
        self.U_o = torch.rand(hidden_size, hidden_size).to(self.device)
        self.b_o = torch.rand(hidden_size).to(self.device)

    def forward(self, x):
        batch_size = x.size(0)
        sequence_length = x.size(1)
        hidden_sequence = []

        hx = torch.zeros(batch_size, self.hidden_size).to(self.device)
        cx = torch.zeros(batch_size, self.hidden_size).to(self.device)            

        for t in range(sequence_length):
            x_t = x[:, t, :]  
            
            forget_gate = torch.sigmoid(torch.mm(x_t, self.W_f) + torch.mm(hx, self.U_f) + self.b_f)
            input_gate = torch.sigmoid(torch.mm(x_t, self.W_i) + torch.mm(hx, self.U_i) + self.b_i)
            cell_gate = torch.tanh(torch.mm(x_t, self.W_c) + torch.mm(hx, self.U_c) + self.b_c)
            output_gate = torch.sigmoid(torch.mm(x_t, self.W_o) + torch.mm(hx, self.U_o) + self.b_o)

            cx = forget_gate * cx + input_gate * cell_gate
            hx = output_gate * torch.tanh(cx)

            hidden_sequence.append(hx.unsqueeze(0))

        hidden_sequence = torch.cat(hidden_sequence, 0)
        hidden_sequence = hidden_sequence.transpose(0, 1)
        return hidden_sequence
































# Save your trained model
torch.save(model.state_dict(), 'lstm.pth')



